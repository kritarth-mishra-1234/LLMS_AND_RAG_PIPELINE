Document 1:
('URL: https://blog.langchain.dev/announcing-langsmith/\nTitle: Announcing LangSmith, a unified platform for debugging, testing, evaluating, and monitoring your LLM applications\n\nLangChain exists to make it as easy as possible to develop LLM-powered applications.\n\nWe started with an open-source Python package when the main blocker for building LLM-powered applications was getting a simple prototype working. We remember seeing Nat Friedman tweet in late 2022 that there was “not enough tinkering happening.” The LangChain open-source packages are aimed at addressing this and we see lots of tinkering happening now (Nat agrees)–people are building everything from chatbots over internal company documents to an AI dungeon master for a Dungeons and Dragons game.\n\nThe blocker has now changed. While it’s easy to build a prototype of an application in ~5 lines of LangChain code, it’s still deceptively hard to take an application from prototype to production. The main issue that we see today is appli', {'chunk_size': 1000.0, 'source': 'langchain_blog_posts/blog.langchain.dev_announcing-langsmith_.txt'})
----------------------------------------------------------------------------------------------------
Document 2:
("a highly-customized, and highly performant, series of applications on top of LangChain’s framework by relying on this same infrastructure.\n\n“We are proud of being one of the early LangChain design partners and users of LangSmith,” Said Dan Sack, Managing Director and Partner at BCG. “The use of LangSmith has been key to bringing production-ready LLM applications to our clients. LangSmith's ease of integration and intuitive UI enabled us to have an evaluation pipeline up and running very quickly. Additionally, tracing and evaluating the complex agent prompt chains is much easier, reducing the time required to debug and refine our prompts, and giving us the confidence to move to deployment.”\n\nIn another example of debugging in action, we partnered with DeepLearningAI to equip learners in the recently-released LangChain courses with access to LangSmith. This allowed students to easily visualize the exact sequence of calls, and the inputs and outputs at each step in the chain with precision. Students can understand exactly what the chains, prompts, and LLMs were doing, which helps build intuition as they learn to create new and more sophisticated applications.\n\n\n\nTesting\n\nOne of the ma", {'chunk_size': 1200.0, 'source': 'langchain_blog_posts/blog.langchain.dev_announcing-langsmith_.txt'})
----------------------------------------------------------------------------------------------------
Document 3:
('it on our own internal workflows, and we’ve found LangSmith helps teams in 5 core ways:\n\nDebugging\n\nLangSmith gives you full visibility into model inputs and output of every step in the chain of events. This makes it easy for teams to experiment with new chains and prompt templates, and spot the source of unexpected results, errors, or latency issues. We’ll also expose latency and token usage so that you can identify which calls are causing issues.\n\nWe’ve also made it easy to change and rerun examples from the UI. We added this feature after seeing teams take logs of bad examples and copy-paste into the OpenAI playground to tweak the prompt until they got a good result. We wanted to eliminate that friction, and now with the click of a button, you can go from a log to a playground where you can actively edit. This is currently supported for both OpenAI and Anthropic models, with support for more coming soon. We’re also working on supporting this for chains in general.\n\nThis deep visibility into model performance has been particularly helpful for teams developing complex applications. LangSmith helped Streamlit and Snowflake implement agents that could intelligently and reliably answ', {'chunk_size': 1200.0, 'source': 'langchain_blog_posts/blog.langchain.dev_announcing-langsmith_.txt'})
----------------------------------------------------------------------------------------------------
Document 4:
('GenAI at Theodo and creator of Quivr.\n\n\n\nA unified platform\n\nWhile each of these product areas provide unique value, often at a specific point in time in the development process, we believe a great deal of the long term impact of LangSmith will come from having a single, fully-integrated hub to do this work from. We see teams with all kinds of Rube Goldberg-machine-like processes for managing their LLM applications, and we want to make that a thing of the past.\n\nAs a very simple example, we considered it to be table stakes for LangSmith to help users easily create datasets from existing logs and use them immediately for testing and evaluation, seamlessly connecting the logging/debugging workflows to the testing/evaluation ones.\n\n\n\nFintual, a Latin American startup with big dreams to help their citizens build wealth through a personalized financial advisor, found LangSmith early in their LLM development journey. “As soon as we heard about LangSmith, we moved our entire development stack onto it. We could build the evaluation, testing, and monitoring tools we needed in-house. But it would be 1000x worse, take us 10x longer, and require a team 2x the size,” said Fintual leader Jose Pe', {'chunk_size': 1200.0, 'source': 'langchain_blog_posts/blog.langchain.dev_announcing-langsmith_.txt'})
----------------------------------------------------------------------------------------------------
